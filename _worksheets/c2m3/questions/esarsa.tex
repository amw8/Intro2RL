The update rule for Expected Sarsa is on-policy,
$$ Q(S_{t}, A_{t}) \leftarrow Q(S_{t}, A_{t}) + \alpha \left[  R_{t+1} + \gamma \mathbb{E}_{\pi}[Q(S_{t+1}, A_{t+1}) | S_{t+1}] - Q(S_{t}, A_{t})\right]$$
when the target policy $\pi$ is the same as the behavior policy $b$ that generated $\{S_{t}, A_{t}, R_{t+1}, S_{t+1}\}$.
However, Expected Sarsa can be made to learn off-policy with different target policies $\pi$ and behavior policies $b$.
\begin{enumerate}
  \item To learn off-policy in Monte Carlo learning, we needed to correct the expectation using importance sampling.
    Explain why we not need importance sampling in Expected Sarsa, despite there being an expectation term.
  \item We know that Expected Sarsa generalizes Q-learning, but does it also generalize Sarsa?
    In other words, does there exist a target policy such that the expectation term is equal to $Q(S_{t+1}, A_{t+1})$ where $A_{t+1} \sim b(a' | S_{t+1})$?
\end{enumerate}

%% \textbf{Answer:}

%% \begin{enumerate}
%% \item There is no importance sampling needed in expected SARSA, since we never actually take any of the actions $a$ in $\sum_a \pi(a|S') Q(S', a)$ during the interaction. Here, $a$ is just used to update the estimate of $Q(S, A)$, and never affects the future trajectory.

%%   Consider the state--action value Bellman equation:
%%   \begin{equation*}
%%     q_\pi(s, a) = \sum_{s', r} p(s', r | s, a) \left[ r + \gamma \sum_{a'} \pi(a'|s') q_\pi(s', a') \right].
%%   \end{equation*}

%%   We can see that from the above equation, since the action $a'$ is never actually taken (i.e. it is not part of the sampled trajectory), the above term is equal to the expectation of $R + \gamma q(S, A)$ under the target policy $\pi$. Whereas, if the $a'$ action were taken during the agent--environment interaction ($a' \sim b$, with $b$ as the behavior policy), we would have had to use importance sampling, as in the algorithm $Q(\sigma)$.

%%   \textcolor{red}{Note: In this example, we ignore the stationary distribution and also the distribution of the first action $A$.}
  
%% \item When the target policy is exactly the same as the behavior policy, expected SARSA becomes an on--policy algorithm. For it to exactly reduce to SARSA, one possible way is that the target policy chooses the action $A'$ with probability one in state $S'$, for all the state action pairs encountered in the trajectory. This is possible, if both the behavior and target policies are the same and are deterministic, i.e. choose a specific action with probability one in each state.
%% \end{enumerate}
