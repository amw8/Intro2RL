Suppose there is an MDP that under any policy produces the deterministic
sequence of rewards $+1, 0, +1, 0, +1, 0, \dotso$ going on forever. Technically, this is not allowed
because it violates ergodicity; there is no stationary limiting distribution $\mu_{\pi}$ and the limit
$$\underset{t\rightarrow \infty}{\lim} \mathbb{E}[R_{t} | S_{0}, A_{0:(t-1)} \sim \pi]$$
does not exist. Nevertheless, the average reward,
$$\underset{h\rightarrow \infty} {\lim}\frac{1}{h} \sum_{t=1}^{h}\mathbb{E}[R_{t} | S_{0}, A_{0:(t-1)} \sim \pi]$$
is well defined;
What is it? Now consider two states in this MDP. From A, the reward sequence is exactly as described above, starting with a $+1$, whereas, from B, the reward sequence starts with a $0$ and then continues with $+1, 0, +1, 0,\dotso $ The differential return
$$G_{t} = R_{t+1} - r(\pi) + R_{t+2} - r(\pi) + R_{t+3} - r(\pi) + \, \dotso$$
is not well defined for this case as the limit does not exist. To repair this, one could alternately define the value of a state as

$$ v_{\pi} = \underset{\gamma \rightarrow 1}{\lim}\, \underset{h \rightarrow \infty}{\lim} \sum_{t=0}^{h} \gamma^{t}\left(  \mathbb{E}_{\pi} [R_{t+1} | S_{0} = s] - r(\pi)\right)$$
Under this definition, what are the values of states A and B?